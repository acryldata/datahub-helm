# Values to start up datahub after starting up the datahub-prerequisites chart with "prerequisites" release name
# Copy this chart and change configuration as needed.
datahub-gms:
  enabled: true
  image:
    repository: linkedin/datahub-gms
    tag: "v0.8.44"

datahub-frontend:
  enabled: true
  image:
    repository: linkedin/datahub-frontend-react
    tag: "v0.8.44"
  # Set up ingress to expose react front-end
  ingress:
    enabled: false

acryl-datahub-actions:
  enabled: true
  image:
    repository: acryldata/datahub-actions
    tag: "v0.0.6"
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi

datahub-mae-consumer:
  image:
    repository: linkedin/datahub-mae-consumer
    tag: "v0.8.44"

datahub-mce-consumer:
  image:
    repository: linkedin/datahub-mce-consumer
    tag: "v0.8.44"

datahub-ingestion-cron:
  enabled: false
  image:
    repository: acryldata/datahub-ingestion
    tag: "v0.8.44"

elasticsearchSetupJob:
  enabled: true
  image:
    repository: linkedin/datahub-elasticsearch-setup
    tag: "v0.8.44"
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

kafkaSetupJob:
  enabled: true
  image:
    repository: linkedin/datahub-kafka-setup
    tag: "v0.8.44"
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

mysqlSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-mysql-setup
    tag: "v0.8.44"
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

postgresqlSetupJob:
  enabled: false
  image:
    repository: acryldata/datahub-postgres-setup
    tag: "v0.8.44"
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}

datahubUpgrade:
  enabled: true
  image:
    repository: acryldata/datahub-upgrade
    tag: "v0.8.44"
  batchSize: 1000
  batchDelayMs: 100
  noCodeDataMigration:
    sqlDbType: "MYSQL"
    # sqlDbType: "POSTGRES"
  podSecurityContext: {}
    # fsGroup: 1000
  securityContext: {}
    # runAsUser: 1000
  podAnnotations: {}
  restoreIndices:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 300m
        memory: 256Mi

prometheus-kafka-exporter:
  enabled: false
  kafkaServer:
  - prerequisites-kafka:9092  # <<release-name>>-kafka:9092
  # Sarama logging
  sarama:
    logEnabled: false
  env: []
  # - name: <ENV_VAR_NAME>
  #   value: <value>

  # List of additional cli arguments to configure kafka-exporter
  # for example: --log.enable-sarama, --log.level=debug, etc.
  # all the possible args can be found here: https://github.com/danielqsj/kafka_exporter#flags
  extraArgs: []
  
  prometheus:
  serviceMonitor:
    enabled: false
    namespace: monitoring
    interval: "30s"
    # If serviceMonitor is enabled and you want prometheus to automatically register
    # target using serviceMonitor, add additionalLabels with prometheus release name
    # e.g. If you have deployed kube-prometheus-stack with release name kube-prometheus
    # then additionalLabels will be
    # additionalLabels:
    #   release: kube-prometheus
    additionalLabels: {}
    targetLabels: []

  # this allows the usage of tls connection to your kafka cluster based on a secret in tls format.
  # Enable it if you are connecting to kafka cluster using tls connection or sasl_ssl
  # mandatory keys:
  # ca.crt
  # tls.crt
  # tls.key
  tls:
    enabled: false
    insecureSkipVerify: true
  # The kafka server's name. Used to verify the hostname on the returned certificates unless tls.insecureSkipVerify is set to true.
    serverName: ""
  # Secret containing ca.crt, tls.crt and tls.key Keys
  # kubectl create secret generic <name of an existing secret> --from-file=tls.crt=./tls.crt --from-file=tls.key=./tls.key --from-literal=ca.crt=./ca.crt 
  #  mountPath: /secret/certs
  #  secretName: <name of an existing secret>

  sasl:
    enabled: false
    handshake: true
    scram:
      enabled: false
      # mechanism: <plain|scram-sha512|scram-sha256>   
      # add username and password
      # username:
      # password:
      # or use an existing secret
      # secretName: <name of an existing secret>
    kerberos:
      enabled: false
      # serviceName:
      # realm:
      # kerberosAuthType: <keytabAuth|userAuth>
      # mountPath: /secret/kerberos
      # secretName: <name of an existing secret>  

global:
  graph_service_impl: neo4j
  datahub_analytics_enabled: true
  datahub_standalone_consumers_enabled: false

  elasticsearch:
    host: "elasticsearch-master"
    port: "9200"

  kafka:
    bootstrap:
      server: "prerequisites-kafka:9092"
    zookeeper:
      server: "prerequisites-zookeeper:2181"
    ## For AWS MSK set this to a number larger than 1
    # partitions: 3
    # replicationFactor: 3
    schemaregistry:
      url: "http://prerequisites-cp-schema-registry:8081"
      # type: AWS_GLUE
      # glue:
      #   region: us-east-1
      #   registry: datahub

  neo4j:
    host: "prerequisites-neo4j-community:7474"
    uri: "bolt://prerequisites-neo4j-community"
    username: "neo4j"
    password:
      secretRef: neo4j-secrets
      secretKey: neo4j-password

  sql:
    datasource:
      host: "prerequisites-mysql:3306"
      hostForMysqlClient: "prerequisites-mysql"
      port: "3306"
      url: "jdbc:mysql://prerequisites-mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2"
      driver: "com.mysql.cj.jdbc.Driver"
      username: "root"
      password:
        secretRef: mysql-secrets
        secretKey: mysql-root-password

      ## Use below for usage of PostgreSQL instead of MySQL
      # host: "prerequisites-postgresql:5432"
      # hostForpostgresqlClient: "prerequisites-postgresql"
      # port: "5432"
      # url: "jdbc:postgresql://prerequisites-postgresql:5432/datahub"
      # driver: "org.postgresql.Driver"
      # username: "postgres"
      # password:
      #   secretRef: postgresql-secrets
      #   secretKey: postgres-password

  datahub:
    gms:
      port: "8080"
      nodePort: "30001"

    monitoring:
      enablePrometheus: true

    mae_consumer:
      port: "9091"
      nodePort: "30002"

    appVersion: "1.0"

    encryptionKey:
      secretRef: "datahub-encryption-secrets"
      secretKey: "encryption_key_secret"
      # Set to false if you'd like to provide your own secret.
      provisionSecret: true

    managed_ingestion:
      enabled: true
      defaultCliVersion: "0.8.44"

    metadata_service_authentication:
      enabled: false
      systemClientId: "__datahub_system"
      systemClientSecret:
        secretRef: "datahub-auth-secrets"
        secretKey: "token_service_signing_key"
      tokenService:
        signingKey:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_signing_key"
        salt:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_salt"
      # Set to false if you'd like to provide your own auth secrets
      provisionSecrets: true

#  hostAliases:
#    - ip: "192.168.0.104"
#      hostnames:
#        - "broker"
#        - "mysql"
#        - "postgresql"
#        - "elasticsearch"
#        - "neo4j"

## Add below to enable SSL for kafka
#  credentialsAndCertsSecrets:
#    name: datahub-certs
#    path: /mnt/datahub/certs
#    secureEnv:
#      ssl.key.password: datahub.linkedin.com.KeyPass
#      ssl.keystore.password: datahub.linkedin.com.KeyStorePass
#      ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#      kafkastore.ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#
#  springKafkaConfigurationOverrides:
#    ssl.keystore.location: /mnt/datahub/certs/datahub.linkedin.com.keystore.jks
#    ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    kafkastore.ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    security.protocol: SSL
#    kafkastore.security.protocol: SSL
#    ssl.keystore.type: JKS
#    ssl.truststore.type: JKS
#    ssl.protocol: TLS
#    ssl.endpoint.identification.algorithm:
