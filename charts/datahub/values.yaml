# Values to start up datahub after starting up the datahub-prerequisites chart with "prerequisites" release name
# Copy this chart and change configuration as needed.
datahub-gms:
  enabled: true
  image:
    repository: acryldata/datahub-gms
    # tag: "v0.11.0 # defaults to .global.datahub.version
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  resources:
    limits:
      memory: 2Gi
    requests:
      cpu: 100m
      memory: 1Gi
  livenessProbe:
    initialDelaySeconds: 60
    periodSeconds: 30
    failureThreshold: 8
  readinessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  theme_v2:
    enabled: true
    default: true
    toggeable: true
  # Optionaly specify service type for datahub-gms: LoadBalancer, ClusterIP or NodePort, by default: LoadBalancer
  # service:
  #   type: ClusterIP
  # Load balancer class for datahub-gms. Used by cloud providers to select a load balancer implementation other than the cloud provider default.
  #   loadBalancerClass: ""
  # Optionally set a GMS specific SQL login (defaults to global login)
  # sql:
  #   datasource:
  #     username: "gms-login"
  #     password:
  #       secretRef: gms-secret
  #       secretKey: gms-password

datahub-frontend:
  enabled: true
  replicaCount: 1
  hpa:
    # If enabled, Horizontal Pod Autoscaler (HPA) will automatically scale the number of replicas based on CPU and memory utilization thresholds.
    enabled: false
    minReplicas: 2
    maxReplicas: 3
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Pods
            value: 1
            periodSeconds: 180
      scaleUp:
        stabilizationWindowSeconds: 0
        policies:
          - type: Pods
            value: 1
            periodSeconds: 60
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 70
  image:
    repository: acryldata/datahub-frontend-react
    # tag: "v0.11.0" # # defaults to .global.datahub.version
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  resources:
    limits:
      memory: 1400Mi
    requests:
      cpu: 100m
      memory: 512Mi
  # Set up ingress to expose react front-end
  ingress:
    enabled: false
    # className: ""
    # extraLabels: {}
    # annotations: {}
    # hosts:
    #   - host: chart-example.local
    #     paths: []
    #     redirectPaths: []
    # tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
    # pathType: ImplementationSpecific
  defaultUserCredentials: {}
    # randomAdminPassword: true
    # You can also set specific passwords for default users
    # manualValues: |
    #   datahub:manualPassword
    #   initialViewer:manualPassword
  service:
    extraLabels: {}
  # Optionaly specify service type for datahub-frontend: LoadBalancer, ClusterIP or NodePort, by default: LoadBalancer
  #   type: ClusterIP

acryl-datahub-actions:
  enabled: true
  image:
    repository: acryldata/datahub-actions
    # tag: "v1.2.0" # # defaults to .global.datahub.version
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  # mount the k8s secret as a volume in the container, each key name is mounted as a file on the mount path /etc/datahub/ingestion-secret-files
  # ingestionSecretFiles:
  #   name: ${K8S_SECRET_NAME}
  #   defaultMode: "0444"
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi

datahub-mae-consumer:
  image:
    repository: acryldata/datahub-mae-consumer
    # tag: "v0.11.0" # defaults to .global.datahub.version
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  resources:
    limits:
      memory: 1536Mi
    requests:
      cpu: 100m
      memory: 256Mi

datahub-mce-consumer:
  image:
    repository: acryldata/datahub-mce-consumer
    # tag: "v0.11.0" # defaults to .global.datahub.version
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  resources:
    limits:
      memory: 1536Mi
    requests:
      cpu: 100m
      memory: 256Mi

datahub-ingestion-cron:
  enabled: false
  image:
    repository: acryldata/datahub-ingestion
    # tag: "v0.11.0" # defaults to .global.datahub.version

elasticsearchSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-elasticsearch-setup
    # tag: "v0.11.0" # defaults to .global.datahub.version
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  # Custom labels to add to the Job metadata
  labels: {}
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # If you want to use OpenSearch instead of ElasticSearch add the USE_AWS_ELASTICSEARCH environment variable below
  # extraEnvs:
  #   - name: USE_AWS_ELASTICSEARCH
  #     value: "true"
  # Add extra sidecar containers to job pod
  extraSidecars: []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always

kafkaSetupJob:
  enabled: false
  image:
    repository: acryldata/datahub-kafka-setup
    tag: v1.2.0.1
    # tag: "v0.11.0" # defaults to .global.datahub.version
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  # Custom labels to add to the Job metadata
  labels: {}
  resources:
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 300m
      memory: 768Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Add extra sidecar containers to job pod
  extraSidecars: []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always

mysqlSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-mysql-setup
    # tag: "v0.11.0" # defaults to .global.datahub.version
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  # Custom labels to add to the Job metadata
  labels: {}
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Optionally set a set-up job specific login (defaults to global login)
  # username: "mysqlSetupJob-login"
  # password:
  #   secretRef: mysqlSetupJob-secret
  #   secretKey: mysqlSetupJob-password
  # Add extra sidecar containers to job pod
  extraSidecars: []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always

postgresqlSetupJob:
  enabled: false
  image:
    repository: acryldata/datahub-postgres-setup
    # tag: "v0.11.0" # defaults to .global.datahub.version
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  # Custom labels to add to the Job metadata
  labels: {}
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Optionally set a set-up job specific login (defaults to global login)
  # username: "postgresqlSetupJob-login"
  # password:
  #   secretRef: postgresqlSetupJob-secret
  #   secretKey: postgresqlSetupJob-password
  # Add extra sidecar containers to job pod
  extraSidecars: []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always
  # Optionally set a specific database using extraEnvs
  # extraEnvs: []
  #   - name: "DATAHUB_DB_NAME"
  #     value: "dh"

## No code data migration
datahubUpgrade:
  enabled: true
  image:
    repository: acryldata/datahub-upgrade
    # tag: "v0.11.0"  # defaults to .global.datahub.version
  batchSize: 1000
  batchDelayMs: 100
  podSecurityContext: {}
    # fsGroup: 1000
  securityContext: {}
    # runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-weight: "-2"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Add extra sidecar containers to job pod
  extraSidecars: []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always
  restoreIndices:
    # Custom labels to add to the CronJob metadata
    labels: {}
    image:
      # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
      # to run
      command:
      # Overrides the entire set of upgrade job arguments, for if you want to specify exactly whi
      args: []
    # Alternative for overriding specific args instead of changing the entire set and overriding defaults
    args:
      # Restore only rows with less than a certain epoch millisecond timestamp
      lePitEpochMs:
      # Restore only rows with greater than a certain epoch millisecond timestamp
      gePitEpochMs:
      # Used in conjunction with urnBasedPagination, resume from a particular urn
      lastUrn:
      # Used in conjunction with urnBasedPagination and lastUrn, resume from a particular urn for a particular aspect to avoid reprocessing certain aspects
      lastAspect:
      # Uses a key based paging strategy when scrolling through SQL rows instead of offset. Often faster for large sets of data
      urnBasedPagination:
      # Only restore for certain aspects, comma separated list
      aspectNames:
      # A SQL "like" statement that matches against the urn column
      urnLike:
      # Used with default paging, start from a specified offset
      startingOffset:
      # Specifies number of threads for processing pages of rows to restore
      numThreads:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 300m
        memory: 256Mi
    # Schedule of CronJob when enabled
    schedule: "0 0 * * 0"
    # Add the concurrency Policy flexibility via values
    concurrencyPolicy: Allow
    # Add extra sidecar containers to job pod
    extraSidecars: []
      # - name: my-image-name
      #   image: my-image
      #   imagePullPolicy: Always
    # Optionally set a specific PostgreSQL database name using extraEnvs
    # extraEnvs:
    #   - name: "DATAHUB_DB_NAME"
    #    value: "dh"
  extraInitContainers: []

## Code for ES replica tweaking
datahubSystemCronHourly:
  # This feature is not ready yet, DO NOT enable
  # Enabled by default in case resources need to be allocated to be able to run this job
  enabled: false
  # Custom labels to add to the CronJob metadata
  labels: {}
  image:
    repository: 795586375822.dkr.ecr.us-west-2.amazonaws.com/datahub-upgrade
    # tag: "v0.2.0"  # defaults to .global.datahub.version
  podSecurityContext: {}
    # fsGroup: 1000
  securityContext: {}
    # runAsUser: 1000
  podAnnotations: {}
  # Add extra sidecar containers to job pod
  extraSidecars: []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always
  extraInitContainers: []
  jvmOpts:
    XX: ":+ExitOnOutOfMemoryError -XX:MaxRAMPercentage=75.0"

## Runs system update processes
## Includes: Elasticsearch Indices Creation/Reindex (See global.elasticsearch.index for additional configuration)
datahubSystemUpdate:
  # Custom labels to add to the Job metadata
  labels: {}
  # backOffLimit for the system update job (number of retries before job is considered failed)
  backOffLimit: 6
  # useAdmin determines whether to run system update with admin credentials
  useAdmin: true
  # ServiceAccount configuration for the system update job (enables RBAC when running upgrade jobs)
  # Uncomment and set create: true to create a dedicated service account for system-update
  # serviceAccount:
  #   create: true
  #   name: datahub-operator-sa
  #   rbac: true  # Create Role and RoleBinding for this ServiceAccount
  # SQL setup configuration (when enabled, runs database setup as part of system update)
  sql:
    # Override IAM authentication for Ebean (SQL)
    iam:
      enabled: false  # Inherit from global.sql.iam.enabled if not set
    # SQL setup within system-update job (replaces legacy mysqlSetupJob/postgresqlSetupJob when IAM is enabled)
    setup:
      enabled: false            # Enable SQL database setup
      createTables: true        # Create DataHub tables
      createDatabase: true      # Create the database if it doesn't exist
      createUser: false         # Create SQL user (for non-IAM setups)
  # Elasticsearch setup configuration (when enabled, runs ES setup as part of system update)
  elasticsearch:
    # Override IAM authentication for Elasticsearch
    iam:
      enabled: false  # Inherit from global.elasticsearch.iam.enabled if not set
    # Elasticsearch setup within system-update job
    setup:
      enabled: false            # Enable Elasticsearch user creation
      createUser: false         # CREATE_USER_ES env var - create Elasticsearch user
      createUserIamRoleArn: ""  # CREATE_USER_ES_IAM_ROLE_ARN env var - IAM role ARN to map to the user (only if createUser=true)
  image:
    repository: acryldata/datahub-upgrade
    # tag:
    # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
    # to run
    # command: customCommand
    # args: []
  podSecurityContext: {}
    # fsGroup: 1000
  securityContext: {}
    # runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-4"
    helm.sh/hook-delete-policy: before-hook-creation
  # !! Requires version v0.13.0 or greater
  # Split the system update into 2 jobs, one that is blocking the rest of
  # the deployment and the other which is non-blocking. Once the blocking
  # steps are completed, the non-blocking job runs while the rest of the
  # system is starting.
  nonblocking:
    enabled: true
    # Custom labels to add to the nonblocking Job metadata
    labels: {}
    # When mode = 'nonblocking' the nonblocking job should not include the above helm.sh/hook annotations
    annotations:
      # This is what defines this resource as a hook. Without this line, the
      # job is considered part of the release.
      helm.sh/hook: post-install,post-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
    image:
      args:
      # Add custom command / arguments to this job.  Useful if you need a custom startup or shutdown script
      # to run
      # command: customCommand
      # args: []
  # Depends on v0.14.2 or greater
  bootstrapMCPs:
    default:
      value_configs:
        - "datahub.bootstrapMCPs.default.ingestion.version"
        - "datahub.bootstrapMCPs.default.schedule.timezone"
    datahubGC:
      # For information about this recipe https://datahubproject.io/docs/0.14.0/generated/ingestion/sources/datahubgc/#install-the-plugin
      # Generate values.schedule.interval with a jitter window using a generated config
      dailyCronWindow:
        startHour: 18
        endHour: 5
      # dynamic overrides, the output to each function is deepMerged with values
      values_generated_configs:
        - "datahub.systemUpdate.datahubGC.dailyCronWindow"
      # Environment variable containing the json value for the template mcp
      values_env: DATAHUB_GC_BOOTSTRAP_VALUES
      revision_env: DATAHUB_GC_BOOTSTRAP_REVISION
      # Base values for the template mcp
      values:
        ingestion:
          name: datahub-gc
          # # override global.datahub.managed_ingestion.defaultCliVersion
          # version: "0.14.1.7rc2"
        # schedule:
          # # override global.datahub.timezone
          # timezone: "UTC"
          # # override dailyCronWindow
          # interval: "0 1 * * *"
        cleanup_expired_tokens: "false"
        truncate_indices: "true"
        truncate_indices_retention_days: 30
        dataprocess_cleanup:
          retention_days: 30
          delete_empty_data_jobs: "true"
          delete_empty_data_flows: "true"
          hard_delete_entities: "false"
          keep_last_n: 10
        soft_deleted_entities_cleanup:
          retention_days: 30
  podAnnotations: {}
  resources:
    limits:
      cpu: 500m
      memory: 2Gi
    requests:
      cpu: 300m
      memory: 2Gi
  # Add extra sidecar containers to job pod
  extraSidecars: []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always
  extraInitContainers: []

global:
  # Uncomment and customize the following section to add custom labels to your pods
  # podLabels:
  #   name: datahub
  #   environment: production
  strict_mode: true
  graph_service_impl: elasticsearch
  datahub_analytics_enabled: true
  datahub_standalone_consumers_enabled: false
  imageRegistry: "docker.io"

  # Base path configuration for serving DataHub behind a proxy
  # When enabled, DataHub services will be accessible under the specified base paths
  # This allows hosting DataHub under a subpath like /datahub instead of root /
  # When using the INTERNAL schema registry this will also set the basePath path for clients.
  basePath:
    # Enable basepath support
    enabled: false
    # Base path for DataHub frontend (e.g., /datahub)
    # When set, the frontend will be accessible at this path
    frontend: ""
    # Base path for DataHub GMS API (e.g., /datahub)
    # When set, the GMS will be accessible at this path
    # For deployments where GMS won't be exposed via an ingress that can be left empty.
    gms: ""

  # Change Data Capture (CDC) configuration
  # CDC mode provides strict ordering guarantees for metadata changes by generating
  # MCLs from database change events via Debezium instead of directly from GMS.
  # For more information, see: https://datahubproject.io/docs/deploy/configure-cdc
  # NOTE: CDC topic name is configured in kafka.topics.cdc_topic_name
  cdc:
    # Enable CDC mode (disabled by default)
    enabled: false
    # Partitioning key specification (table:column format)
    urnKeySpec: "datahub.metadata_aspect_v2:urn"
    # Auto-configure Debezium connector (recommended false for production)
    configureSource: false
    # Database configuration for CDC
    database:
      # Database type: mysql or postgres
      type: "mysql"
      # CDC database username - can be a direct string value or reference a secret
      username: "datahub_cdc"
      # OR use secret reference:
      # username:
      #   secretRef: mysql-secrets
      #   secretKey: mysql-cdc-username
      # CDC database password - supports both direct value and secret reference
      password:
        secretRef: mysql-secrets
        secretKey: mysql-cdc-password
        # --------------OR----------------
        # value: datahub_cdc
      # MySQL-specific configuration
      serverId: "184001"
      # PostgreSQL-specific configuration
      includeTable: "public.metadata_aspect_v2"
      includeSchema: "public"
    # Debezium/Kafka Connect configuration
    debezium:
      # Debezium connector name
      connectorName: "datahub-cdc-connector"
      # Kafka Connect REST API URL
      kafkaConnectUrl: "http://kafka-connect:8083"
      # Request timeout in milliseconds
      requestTimeout: "10000"
      # Optional: Override connector class for specific database
      # connectorClass: "io.debezium.connector.mysql.MySqlConnector"
      # Optional: Override logical decoding plugin
      # pluginName: "decoderbufs"

  elasticsearch:
    host: "elasticsearch-master"
    # If you want to use OpenSearch instead of ElasticSearch use different hostname below
    # host: "opensearch-cluster-master"
    port: "9200"
    skipcheck: "false"
    insecure: "false"
    useSSL: "false"

    # Auto-detection is supported if you have cluster permissions for getting current cluster info, may cause issues on restrictive deployed managed services
    engineType: "AUTO_DETECT"
    autoDetect: "true"

    # AWS IAM authentication for OpenSearch
    # When enabled, DataHub will use AWS IAM credentials instead of username/password
    # Requires proper IAM roles and policies to be configured
    iam:
      enabled: false  # Sets OPENSEARCH_USE_AWS_IAM_AUTH
    # AWS region for OpenSearch (required when iam.enabled is true)
    # Example: "us-west-2"
    region: ""

    # Elasticsearch/OpenSearch implementation configuration
    # implementation: "opensearch"  # Sets ELASTICSEARCH_IMPLEMENTATION - "elasticsearch" or "opensearch"

    # Authentication
    # auth:
      # username: "elastic"
      # password:
        # Value can be directly specified here, or stored in a secret with secretRef
        # value: "<password>"
        # or
        # secretRef: <secret-ref>
        # secretKey: <secret-key>
    # If you want to specify index prefixes use indexPrefix
    # indexPrefix: "dh"

    ## The following section controls when and how reindexing of elasticsearch indices are performed
    index:
      ## Enable reindexing when mappings change based on the data model annotations
      enableMappingsReindex: true

      ## Enable reindexing when static index settings change.
      ## Dynamic settings which do not require reindexing are not affected
      ## Primarily this should be enabled when re-sharding is necessary for scaling/performance.
      enableSettingsReindex: true

      ## Index settings can be overridden for entity indices or other indices on an index by index basis
      ## Some index settings, such as # of shards, requires reindexing while others, i.e. replicas, do not
      ## The index settings options can be specified as a string or a map (which is converted to a string
      ## representation)
      ##
      ## Non-Entity indices do not require the prefix
      # settingsOverrides: '{"graph_service_v1":{"number_of_shards":"5"},"system_metadata_service_v1":{"number_of_shards":"5"}}'
      ## or
      # settingsOverrides:
      #   graph_service_v1:
      #     number_of_shards: '5'
      #   system_metadata_service_v1:
      #     number_of_shards: '6'
      #
      ## Entity indices do not require the prefix or suffix
      # entitySettingsOverrides: '{"dataset":{"number_of_shards":"10"}}'

      ## The amount of delay between indexing a document and having it returned in queries
      ## Increasing this value can improve performance when ingesting large amounts of data
      # refreshIntervalSeconds: 1

      ## The following options control settings for datahub-upgrade job when creating or reindexing indices
      upgrade:
        ## When reindexing is required, this option will clone the existing index as a backup
        ## The clone indices are not currently managed.
        cloneIndices: true

        ## Typically when reindexing the document counts between the original and destination indices should match.
        ## In some cases reindexing might not be able to proceed due to incompatibilities between a document in the
        ## orignal index and the new index's mappings. This document could be dropped and re-ingested or restored from
        ## the SQL database.
        ##
        ## This setting allows continuing if and only if the cloneIndices setting is also enabled which
        ## ensures a complete backup of the original index is preserved.
        allowDocCountMismatch: false

        ## Only disable for OpenSearch/ElasticSearch clusters with zone awareness, otherwise optimization should be utilized
        ## Will use full replication during reindexing which will make the process slower
        reindexOptimizationEnabled: true

    ## Search related configuration
    search:
      ## Maximum terms in aggregations
      maxTermBucketSize: 20

      ## Configuration around exact matching for search
      exactMatch:
        ## if false will only apply weights, if true will exclude non-exact
        exclusive: false
        ## include prefix exact matches
        withPrefix: true
        ## boost multiplier when exact with case
        exactFactor: 2.0
        ## boost multiplier when exact prefix
        prefixFactor: 1.6
        ## stacked boost multiplier when case mismatch
        caseSensitivityFactor: 0.7
        ## enable exact match on structured search
        enableStructured: true

      ## Configuration for graph service dao
      graph:
        ## graph dao timeout seconds
        timeoutSeconds: 50
        ## graph dao batch size
        batchSize: 1000
        ## graph dao max result size
        maxResult: 10000

      custom: {}
      # custom:
      #   enabled: true
      #   # See documentation: https://datahubproject.io/docs/how/search/#customizing-search
      #   config:

      ## Semantic search configuration (vector similarity search) [BETA]
      ## Note: Only "document" entity type is officially supported. Other entities may work but YMMV.
      ##
      ## Quick Start Examples:
      ##
      ## For OpenAI:
      ##   semantic:
      ##     enabled: true
      ##     vectorDimension: 1536
      ##     provider:
      ##       type: "openai"
      ##       openai:
      ##         apiKey:
      ##           secretRef: "openai-secret"
      ##           secretKey: "api-key"
      ##         model: "text-embedding-3-small"
      ##
      ## For AWS Bedrock:
      ##   semantic:
      ##     enabled: true
      ##     vectorDimension: 1024
      ##     provider:
      ##       type: "aws-bedrock"
      ##       bedrock:
      ##         modelId: "cohere.embed-english-v3"
      ##         awsRegion: "us-west-2"
      ##
      semantic:
        ## Enable semantic search functionality
        ## Requires documents with embeddings and OpenSearch k-NN support
        enabled: false  # Sets SEARCH_SERVICE_SEMANTIC_SEARCH_ENABLED and ELASTICSEARCH_SEMANTIC_SEARCH_ENABLED

        ## Comma-separated list of entity names for which to create semantic search indices
        ## Only "document" is officially supported. Other entity types (e.g., "chart", "dashboard") are experimental.
        enabledEntities: "document"  # Sets ELASTICSEARCH_SEMANTIC_SEARCH_ENTITIES

        ## Vector dimension - MUST match your embedding model output:
        ## - OpenAI text-embedding-3-small: 1536
        ## - OpenAI text-embedding-3-large: 3072
        ## - AWS Bedrock cohere.embed-english-v3: 1024
        ## - Cohere embed-english-v3.0: 1024
        vectorDimension: 1536  # Sets ELASTICSEARCH_SEMANTIC_VECTOR_DIMENSION

        ## Embedding provider configuration
        provider:
          ## Provider type: "openai", "aws-bedrock", or "cohere"
          type: "openai"  # Sets EMBEDDING_PROVIDER_TYPE

          ## OpenAI configuration (only used when type is "openai")
          openai:
            ## OpenAI API key - recommend using secret reference
            apiKey:
              secretRef: ""  # Name of k8s secret containing OpenAI API key
              secretKey: ""  # Key within secret (e.g., "api-key")
              value: ""  # Alternative: set API key directly (not recommended for production)
            ## OpenAI embedding model - "text-embedding-3-small" (1536d) or "text-embedding-3-large" (3072d)
            model: "text-embedding-3-small"  # Sets OPENAI_EMBEDDING_MODEL
            ## OpenAI API endpoint - can customize for Azure OpenAI or OpenAI-compatible services
            endpoint: "https://api.openai.com/v1/embeddings"  # Sets OPENAI_EMBEDDING_ENDPOINT

          ## AWS Bedrock configuration (only used when type is "aws-bedrock")
          bedrock:
            ## Bedrock model ID - e.g., "cohere.embed-english-v3" (1024 dimensions)
            modelId: "cohere.embed-english-v3"  # Sets EMBEDDING_PROVIDER_MODEL_ID
            ## AWS region where Bedrock is available - e.g., "us-west-2", "us-east-1"
            awsRegion: "us-west-2"  # Sets EMBEDDING_PROVIDER_AWS_REGION

          ## Cohere configuration (only used when type is "cohere")
          cohere:
            ## Cohere API key
            apiKey:
              secretRef: ""  # Name of k8s secret containing Cohere API key
              secretKey: ""  # Key within secret
              value: ""  # Alternative: set API key directly (not recommended)
            ## Cohere embedding model
            model: "embed-english-v3.0"  # Sets COHERE_EMBEDDING_MODEL
            ## Cohere API endpoint
            endpoint: "https://api.cohere.ai/v1/embed"  # Sets COHERE_EMBEDDING_ENDPOINT
      #     # Notes:
      #     #
      #     # First match wins
      #     #
      #     # queryRegex = Java regex syntax
      #     #
      #     # functionScores - See the following for function score syntax
      #     # https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-function-score-query.html
      #     queryConfigurations:
      #       # Select *
      #       - queryRegex: '[*]|'
      #         simpleQuery: false
      #         prefixMatchQuery: false
      #         exactMatchQuery: false
      #         functionScore:
      #           functions:
      #             - filter:
      #                 term:
      #                   deprecated:
      #                     value: true
      #               weight: 0.5
      #           score_mode: multiply
      #           boost_mode: multiply
      #       # Criteria for exact-match only
      #       # Contains quoted or contains underscore then use exact match query
      #       - queryRegex: >-
      #           ["'].+["']|\S+_\S+
      #         simpleQuery: false
      #         prefixMatchQuery: true
      #         exactMatchQuery: true
      #         functionScore:
      #           functions:
      #             - filter:
      #                 term:
      #                   deprecated:
      #                     value: true
      #               weight: 0.5
      #           score_mode: multiply
      #           boost_mode: multiply
      #       # default
      #       - queryRegex: .*
      #         simpleQuery: true
      #         prefixMatchQuery: true
      #         exactMatchQuery: true
      #         functionScore:
      #           functions:
      #             - filter:
      #                 term:
      #                   deprecated:
      #                     value: true
      #               weight: 0.5
      #           score_mode: multiply
      #           boost_mode: multiply

  kafka:
    bootstrap:
      server: "prerequisites-kafka:9092"

    # This is no longer used with  datahub version 1.3.0 and later where datahub system update performs the kafka setup
    # operations if configured. This is retained for legacy reasons for datahub versions before 1.3.0 that still need
    # kafkaSetupJob.enabled = true (current default is kafkaSetupJob.enabled = false)
    zookeeper:
      server: "prerequisites-zookeeper:2181"

    # Set to false if you don't want datahub to create topics during install/upgrade.
    precreateTopics: true

    # This section defines the names for the kafka topics that DataHub depends on, at a global level. Do not override this config
    # at a sub-chart level.
    topics:
      metadata_change_event_name: "MetadataChangeEvent_v4"
      failed_metadata_change_event_name: "FailedMetadataChangeEvent_v4"
      metadata_audit_event_name: "MetadataAuditEvent_v4"
      datahub_usage_event_name: "DataHubUsageEvent_v1"
      metadata_change_proposal_topic_name: "MetadataChangeProposal_v1"
      failed_metadata_change_proposal_topic_name: "FailedMetadataChangeProposal_v1"
      metadata_change_log_versioned_topic_name: "MetadataChangeLog_Versioned_v1"
      metadata_change_log_timeseries_topic_name: "MetadataChangeLog_Timeseries_v1"
      platform_event_topic_name: "PlatformEvent_v1"
      datahub_upgrade_history_topic_name: "DataHubUpgradeHistory_v1"
      cdc_topic_name: "datahub.metadata_aspect_v2"
    consumer_groups:
      datahub_upgrade_history_kafka_consumer_group_id: {}
      #   gms: "<<release-name>>-duhe-consumer-job-client-gms"
      #   mae-consumer: "<<release-name>>-duhe-consumer-job-client-mcl"
      #   mce-consumer: "<<release-name>>-duhe-consumer-job-client-mcp"
      datahub_actions_doc_propagation_consumer_group_id: "datahub_doc_propagation_action"
      datahub_actions_ingestion_executor_consumer_group_id: "ingestion_executor"
      datahub_actions_slack_consumer_group_id: "datahub_slack_action"
      datahub_actions_teams_consumer_group_id: "datahub_teams_action"
      datahub_usage_event_kafka_consumer_group_id: "datahub-usage-event-consumer-job-client"
      metadata_change_log_kafka_consumer_group_id: "generic-mae-consumer-job-client"
      platform_event_kafka_consumer_group_id: "generic-platform-event-job-client"
      metadata_change_event_kafka_consumer_group_id: "mce-consumer-job-client"
      metadata_change_proposal_kafka_consumer_group_id: "generic-mce-consumer-job-client"
    metadataChangeLog:
      hooks:
        siblings:
          enabled: true
          consumerGroupSuffix: ''
        updateIndices:
          enabled: true
          consumerGroupSuffix: ''
        ingestionScheduler:
          enabled: true
          consumerGroupSuffix: ''
        incidents:
          enabled: true
          consumerGroupSuffix: ''
        entityChangeEvents:
          enabled: true
          consumerGroupSuffix: ''
        forms:
          enabled: true
          consumerGroupSuffix: ''
    maxMessageBytes: "5242880"  # 5MB
    producer:
      compressionType: none
      maxRequestSize: "5242880"  # 5MB
    consumer:
      maxPartitionFetchBytes: "5242880"  # 5MB
      stopContainerOnDeserializationError: true
    ## For AWS MSK set this to a number larger than 1
    # partitions: 3
    # replicationFactor: 3
    schemaregistry:
      # GMS Implementation - `url` configured based on component context
      type: INTERNAL
      # Confluent Kafka Implementation
      # type: KAFKA
      # url: "http://prerequisites-cp-schema-registry:8081"

      # Glue Implementation - `url` not applicable
      # type: AWS_GLUE
      # glue:
      #   region: us-east-1
      #   registry: datahub

      # Optional override on whether to configure schema registry clean up policy.
      # When schema registry type is set to INTERNAL, AWS_GLUE, the configureCleanupPolicy is always set to false.
      # This is used *only* if schemaregistry.type is set to KAFKA *and* datahub can create/configure
      # topics (precreateTopics: true).
      # When using a schema registry that datahub cannot configure, explicitly set this flag to false.
      configureCleanupPolicy: true

    # AWS IAM authentication for MSK
    # When enabled, DataHub will use AWS IAM credentials instead of username/password
    # Applies to all components that use Kafka (both Java/Spring and Python services).
    iam:
      enabled: false  # Sets SPRING_KAFKA_PROPERTIES_SASL_* environment variables for Java services
                      # and KAFKA_PROPERTIES_* environment variables for Python services
      awsRegion: ""   # AWS region where MSK cluster is located, e.g. "us-west-2"

  neo4j:
    host: "prerequisites-neo4j:7474"
    uri: "bolt://prerequisites-neo4j"
    username: "neo4j"
    password:
      secretRef: neo4j-secrets
      secretKey: neo4j-password
    # --------------OR----------------
    # value: password

  sql:
    datasource:
      host: "prerequisites-mysql:3306"
      hostForMysqlClient: "prerequisites-mysql"
      port: "3306"
      url: "jdbc:mysql://prerequisites-mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2"
      driver: "com.mysql.cj.jdbc.Driver"
      username: "root"
      password:
        secretRef: mysql-secrets
        secretKey: mysql-root-password
      # --------------OR----------------
      # value: password

      ## Use below for usage of PostgreSQL instead of MySQL
      # host: "prerequisites-postgresql:5432"
      # hostForpostgresqlClient: "prerequisites-postgresql"
      # port: "5432"
      # url: "jdbc:postgresql://prerequisites-postgresql:5432/datahub"
      # driver: "org.postgresql.Driver"
      # username: "postgres"
      # password:
      #   secretRef: postgresql-secrets
      #   secretKey: postgres-password
      # --------------OR----------------
      #   value: password

      # If you want to use specific PostgreSQL database use extraEnvs
      # extraEnvs:
      #  - name: "DATAHUB_DB_NAME"
      #    value: "dh"

      ## Use below for usage of Google Cloud SQL MySQL instance instead of the self hosted MySQL,
      ## make sure you have deployed gcloud-sqlproxy as prerequisite
      # host: "prerequisites-gcloud-sqlproxy:3306"
      # hostForMysqlClient: "prerequisites-gcloud-sqlproxy"
      # port: "3306"
      # url: "jdbc:mysql://prerequisites-gcloud-sqlproxy:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2"
      # driver: "com.mysql.cj.jdbc.Driver"
      # username: "root"
      # password:
      #   secretRef: mysql-secrets
      #   secretKey: mysql-root-password
      # --------------OR----------------
      #   value: password

    # IAM authentication for SQL databases (supports both MySQL and PostgreSQL)
    # When enabled, DataHub will use IAM credentials instead of username/password
    # Requires proper IAM roles and policies to be configured
    iam:
      enabled: false  # Sets EBEAN_USE_IAM_AUTH and EBEAN_POSTGRES_USE_AWS_IAM_AUTH
      cloudProvider: "auto"  # Cloud provider: "auto" (detect), "aws", or "gcp"
      awsRegion: ""  # AWS region for RDS/Aurora IAM authentication

  datahub:
    version: v1.3.0.1
    gms:
      protocol: "http"
      port: "8080"
      nodePort: "30001"

    # Used for scheduled tasks
    timezone: "UTC"

    frontend:
      validateSignUpEmail: true

    monitoring:
      enablePrometheus: true
      # Set a custom name for the monitoring port
      portName: jmx

    mae_consumer:
      port: "9091"
      nodePort: "30002"

    appVersion: "1.0.0"
    systemUpdate:
      ## The following options control settings for datahub-upgrade job which will
      ## managed ES indices and other update related work
      enabled: true

    encryptionKey:
      secretRef: "datahub-encryption-secrets"
      secretKey: "encryption_key_secret"
      # Set to false if you'd like to provide your own secret.
      provisionSecret:
        enabled: true
        autoGenerate: true
        annotations: {}
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    encryptionKey: <encryption key value>

    managed_ingestion:
      enabled: true
      defaultCliVersion: "1.0.0"

    metadata_service_authentication:
      enabled: true
      systemClientId: "__datahub_system"
      systemClientSecret:
        secretRef: "datahub-auth-secrets"
        secretKey: "system_client_secret"
      tokenService:
        signingKey:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_signing_key"
        salt:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_salt"
      # Set to false if you'd like to provide your own auth secrets
      provisionSecrets:
        enabled: true
        autoGenerate: true
        annotations: {}
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    secret: <secret value>
      #    signingKey: <signing key value>
      #    salt: <salt value>

    ## Enables always emitting a MCL even when no changes are detected. Used for Time Based Lineage when no changes occur.
    alwaysEmitChangeLog: false

    ## Enables diff mode for graph writes, uses a different code path that produces a diff from previous to next to write relationships instead of wholesale deleting edges and reading
    enableGraphDiffMode: true

    ## Enable stricter URN validation logic
    strictUrnValidation: false

    ## Values specific to the unified search and browse feature.
    search_and_browse:
      show_search_v2: true  # If on, show the new search filters experience as of v0.10.5
      show_browse_v2: true  # If on, show the new browse experience as of v0.10.5
      backfill_browse_v2: true  # If on, run the backfill upgrade job that generates default browse paths for relevant entities

    ## v0.15.0+
    metadataChangeProposal:
      consumer:
        batch:
          enabled: false


    ## v0.13.4+
    mcp:
      throttle:
        # updateIntervalMs: 60000
        mceConsumer:  # v0.14.2+
          enabled: false
        apiRequests:  # v0.14.2+
          enabled: false
        ## Versioned MCL topic
        versioned:
          ## Whether to throttle MCP processing based on MCL backlog
          enabled: true
        #  threshold: 4000
        #  maxAttempts: 1000
        #  initialIntervalMs: 100
        #  multiplier: 10
        #  maxIntervalMs:  30000
        # Timeseries MCL topic
        timeseries:
          ## Whether to throttle MCP processing based on MCL backlog
          enabled: false
        #  threshold: 4000
        #  maxAttempts: 1000
        #  initialIntervalMs: 100
        #  multiplier: 10
        #  maxIntervalMs: 30000

    ## v0.15.0+
    entityVersioning:
      enabled: false

    ## Enables a fast path for processing events sourced from the UI, updates indices synchronously for requests originating from GraphQL
    preProcessHooksUIEnabled: true
    ## Reprocess UI events at MAE Consumer, normally if preprocess is enabled this is not required.
    reProcessUIEventHooks: false

#  hostAliases:
#    - ip: "192.168.0.104"
#      hostnames:
#        - "broker"
#        - "mysql"
#        - "postgresql"
#        - "elasticsearch"
#        - "neo4j"

## Add below to enable SSL for kafka
#  credentialsAndCertsSecrets:
#    name: datahub-certs
#    path: /mnt/datahub/certs
#    secureEnv:
#      ssl.key.password: datahub.linkedin.com.KeyPass
#      ssl.keystore.password: datahub.linkedin.com.KeyStorePass
#      ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#      kafkastore.ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#
#  springKafkaConfigurationOverrides:
#    ssl.keystore.location: /mnt/datahub/certs/datahub.linkedin.com.keystore.jks
#    ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    kafkastore.ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    security.protocol: SSL
#    kafkastore.security.protocol: SSL
#    ssl.keystore.type: JKS
#    ssl.truststore.type: JKS
#    ssl.protocol: TLS
#    ssl.endpoint.identification.algorithm:
#    basic.auth.credentials.source: USER_INFO
#    basic.auth.user.info:
