# Values to start up datahub after starting up the datahub-prerequisites chart with "prerequisites" release name
# Copy this chart and change configuration as needed.
datahub-gms:
  enabled: true
  image:
    repository: linkedin/datahub-gms
    # tag: "v0.10.0 # defaults to .global.datahub.version
  resources:
    limits:
      memory: 2Gi
    requests:
      cpu: 100m
      memory: 1Gi
  # Optionally set a GMS specific SQL login (defaults to global login)
  # sql:
  #   datasource:
  #     username: "gms-login"
  #     password:
  #       secretRef: gms-secret
  #       secretKey: gms-password

datahub-frontend:
  enabled: true
  image:
    repository: linkedin/datahub-frontend-react
    # tag: "v0.10.0" # # defaults to .global.datahub.version
  resources:
    limits:
      memory: 1400Mi
    requests:
      cpu: 100m
      memory: 512Mi
  # Set up ingress to expose react front-end
  ingress:
    enabled: false

acryl-datahub-actions:
  enabled: true
  image:
    repository: acryldata/datahub-actions
    tag: "v0.0.11"
  # mount the k8s secret as a volume in the container, each key name is mounted as a file on the mount path /etc/datahub/ingestion-secret-files
  # ingestionSecretFiles:
  #   name: ${K8S_SECRET_NAME}
  #   defaultMode: "0444"
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi

datahub-mae-consumer:
  image:
    repository: linkedin/datahub-mae-consumer
    # tag: "v0.10.0" # defaults to .global.datahub.version
  resources:
    limits:
      memory: 1536Mi
    requests:
      cpu: 100m
      memory: 256Mi

datahub-mce-consumer:
  image:
    repository: linkedin/datahub-mce-consumer
    # tag: "v0.10.0" # defaults to .global.datahub.version
  resources:
    limits:
      memory: 1536Mi
    requests:
      cpu: 100m
      memory: 256Mi

datahub-ingestion-cron:
  enabled: false
  image:
    repository: acryldata/datahub-ingestion
    # tag: "v0.10.0" # defaults to .global.datahub.version

elasticsearchSetupJob:
  enabled: true
  image:
    repository: linkedin/datahub-elasticsearch-setup
    # tag: "v0.10.0" # defaults to .global.datahub.version
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}
  initContainers:
    - name: wait-for-elasticsearch
      image: "jwilder/dockerize:latest"
      command:
        - /bin/sh
        - -c
        - |
          export elasticsearchUrl={{ printf "%s:%s" .Values.global.elasticsearch.host .Values.global.elasticsearch.port }}
          dockerize -wait "tcp://${elasticsearchUrl}/" -timeout 360s

kafkaSetupJob:
  enabled: true
  image:
    repository: linkedin/datahub-kafka-setup
    # tag: "v0.10.0" # defaults to .global.datahub.version
  resources:
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 300m
      memory: 768Mi
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}
  initContainers:
    - name: wait-for-kafka
      image: "jwilder/dockerize:latest"
      command:
        - /bin/sh
        - -c
        - dockerize -wait "tcp://{{ .Values.global.kafka.bootstrap.server | quote }}/" -timeout 360s

mysqlSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-mysql-setup
    # tag: "v0.10.0" # defaults to .global.datahub.version
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}
  # Optionally set a set-up job specific login (defaults to global login)
  # username: "mysqlSetupJob-login"
  # password:
  #   secretRef: mysqlSetupJob-secret
  #   secretKey: mysqlSetupJob-password
  initContainers:
    - name: wait-for-mysql
      image: "jwilder/dockerize:latest"
      command:
        - /bin/sh
        - -c
        - |
          mysqlUrl={{ printf "%s:%s" .Values.global.sql.datasource.hostForMysqlClient .Values.global.sql.datasource.port }}
          dockerize -wait "tcp://${mysqlUrl}/" -timeout 360s

postgresqlSetupJob:
  enabled: false
  image:
    repository: acryldata/datahub-postgres-setup
    # tag: "v0.10.0" # defaults to .global.datahub.version
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  podAnnotations: {}
  # Optionally set a set-up job specific login (defaults to global login)
  # username: "postgresqlSetupJob-login"
  # password:
  #   secretRef: postgresqlSetupJob-secret
  #   secretKey: postgresqlSetupJob-password
  initContainers:
    - name: wait-for-postgresql
      image: "jwilder/dockerize:latest"
      command:
        - /bin/sh
        - -c
        - |
          export postgresqlUrl={{ printf "%s:%s" .Values.global.sql.datasource.hostForpostgresqlClient .Values.global.sql.datasource.port }}
          dockerize -wait "tcp://${postgresqlUrl}/" -timeout 360s

## No code data migration
datahubUpgrade:
  enabled: true
  cronJobs:
    enabled: true
  image:
    repository: acryldata/datahub-upgrade
    # tag: "v0.10.0"  # defaults to .global.datahub.version
  batchSize: 1000
  batchDelayMs: 100
  podSecurityContext: {}
    # fsGroup: 1000
  securityContext: {}
    # runAsUser: 1000
  podAnnotations: {}
  restoreIndices:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 300m
        memory: 256Mi
  initContainers:
    - name: wait-for-datahub-gms
      image: curlimages/curl:latest
      command:
        - sh
        - -c
        - |
          export datahubGmsHost={{ printf "%s:%s" (printf "%s-%s" .Release.Name "datahub-gms") "8080" }}
          until curl -I --connect-timeout 5 http://${datahubGmsHost}/health; do
            echo "Waiting for http://${datahubGmsHost}/health";
          done;

## Runs system update processes
## Includes: Elasticsearch Indices Creation/Reindex (See global.elasticsearch.index for additional configuration)
datahubSystemUpdate:
  image:
    repository: acryldata/datahub-upgrade
    # tag:
  podSecurityContext: {}
    # fsGroup: 1000
  securityContext: {}
    # runAsUser: 1000
  podAnnotations: {}
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  initContainers:
    - name: wait-for-elasticsearch-setup
      image: bitnami/kubectl:latest
      command: 
        - sh
        - -c
        - |
          echo "Waiting for {{ .Release.Name }}-elasticsearch-setup-job"
          kubectl wait --for=condition=complete job/{{ .Release.Name }}-elasticsearch-setup-job --timeout=300s
    - name: wait-for-kafka-setup
      image: bitnami/kubectl:latest
      command: 
        - sh
        - -c
        - |
          echo "Waiting for {{ .Release.Name }}-kafka-setup-job"
          kubectl wait --for=condition=complete job/{{ .Release.Name }}-kafka-setup-job --timeout=300s
    - name: wait-for-database-setup
      image: bitnami/kubectl:latest
      command: 
        - sh
        - -c
        - |
          {{- if contains "mysql" .Values.global.sql.datasource.driver }}
          echo "Waiting for {{ .Release.Name }}-mysql-setup-job"
          kubectl wait --for=condition=complete job/{{ .Release.Name }}-mysql-setup-job --timeout=300s
          {{ else if contains "postgresql" .Values.global.sql.datasource.driver }}
          echo "Waiting for {{ .Release.Name }}-postgresql-setup-job"
          kubectl wait --for=condition=complete job/{{ .Release.Name }}-postgresql-setup-job --timeout=300s
          {{- end }}

global:
  strict_mode: true
  graph_service_impl: elasticsearch
  datahub_analytics_enabled: true
  datahub_standalone_consumers_enabled: false

  elasticsearch:
    host: "elasticsearch-master"
    port: "9200"
    skipcheck: "false"
    insecure: "false"
    useSSL: "false"

    ## The following section controls when and how reindexing of elasticsearch indices are performed
    index:
      ## Enable reindexing when mappings change based on the data model annotations
      enableMappingsReindex: true

      ## Enable reindexing when static index settings change.
      ## Dynamic settings which do not require reindexing are not affected
      ## Primarily this should be enabled when re-sharding is necessary for scaling/performance.
      enableSettingsReindex: true

      ## Index settings can be overridden for entity indices or other indices on an index by index basis
      ## Some index settings, such as # of shards, requires reindexing while others, i.e. replicas, do not
      ## Non-Entity indices do not require the prefix
      # settingsOverrides: '{"graph_service_v1":{"number_of_shards":"5"},"system_metadata_service_v1":{"number_of_shards":"5"}}'
      ## Entity indices do not require the prefix or suffix
      # entitySettingsOverrides: '{"dataset":{"number_of_shards":"10"}}'

      ## The amount of delay between indexing a document and having it returned in queries
      ## Increasing this value can improve performance when ingesting large amounts of data
      # refreshIntervalSeconds: 1

      ## The following options control settings for datahub-upgrade job when creating or reindexing indices
      upgrade:
        ## When reindexing is required, this option will clone the existing index as a backup
        ## The clone indices are not currently managed.
        cloneIndices: true

        ## Typically when reindexing the document counts between the original and destination indices should match.
        ## In some cases reindexing might not be able to proceed due to incompatibilities between a document in the
        ## orignal index and the new index's mappings. This document could be dropped and re-ingested or restored from
        ## the SQL database.
        ##
        ## This setting allows continuing if and only if the cloneIndices setting is also enabled which
        ## ensures a complete backup of the original index is preserved.
        allowDocCountMismatch: false

    ## Search related configuration
    search:
      ## Maximum terms in aggregations
      maxTermBucketSize: 20

      ## Configuration around exact matching for search
      exactMatch:
        ## if false will only apply weights, if true will exclude non-exact
        exclusive: false
        ## include prefix exact matches
        withPrefix: true
        ## boost multiplier when exact with case
        exactFactor: 2.0
        ## boost multiplier when exact prefix
        prefixFactor: 1.6
        ## stacked boost multiplier when case mismatch
        caseSensitivityFactor: 0.7
        ## enable exact match on structured search
        enableStructured: true

      ## Configuration for graph service dao
      graph:
        ## graph dao timeout seconds
        timeoutSeconds: 50
        ## graph dao batch size
        batchSize: 1000
        ## graph dao max result size
        maxResult: 10000

  kafka:
    bootstrap:
      server: "datahub-kafka:9092"
    zookeeper:
      server: "datahub-zookeeper:2181"
    # This section defines the names for the kafka topics that DataHub depends on, at a global level. Do not override this config
    # at a sub-chart level.
    topics:
      metadata_change_event_name: "MetadataChangeEvent_v4"
      failed_metadata_change_event_name: "FailedMetadataChangeEvent_v4"
      metadata_audit_event_name: "MetadataAuditEvent_v4"
      datahub_usage_event_name: "DataHubUsageEvent_v1"
      metadata_change_proposal_topic_name: "MetadataChangeProposal_v1"
      failed_metadata_change_proposal_topic_name: "FailedMetadataChangeProposal_v1"
      metadata_change_log_versioned_topic_name: "MetadataChangeLog_Versioned_v1"
      metadata_change_log_timeseries_topic_name: "MetadataChangeLog_Timeseries_v1"
      platform_event_topic_name: "PlatformEvent_v1"
      datahub_upgrade_history_topic_name: "DataHubUpgradeHistory_v1"
    ## For AWS MSK set this to a number larger than 1
    # partitions: 3
    # replicationFactor: 3
    schemaregistry:
      url: "http://datahub-cp-schema-registry:8081"
      type: KAFKA
      # glue:
      #   region: us-east-1
      #   registry: datahub

  neo4j:
    host: "datahub-neo4j-community:7474"
    uri: "bolt://datahub-neo4j-community"
    username: "neo4j"
    password:
      secretRef: neo4j-secrets
      secretKey: neo4j-password
    # --------------OR----------------
    # value: password

  sql:
    datasource:
      host: "datahub-mysql:3306"
      hostForMysqlClient: "datahub-mysql"
      port: "3306"
      url: "jdbc:mysql://datahub-mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2"
      driver: "com.mysql.cj.jdbc.Driver"
      username: "root"
      password:
        secretRef: mysql-secrets
        secretKey: mysql-root-password
      # --------------OR----------------
      # value: password

      ## Use below for usage of PostgreSQL instead of MySQL
      # host: "datahub-postgresql:5432"
      # hostForpostgresqlClient: "datahub-postgresql"
      # port: "5432"
      # url: "jdbc:postgresql://datahub-postgresql:5432/datahub"
      # driver: "org.postgresql.Driver"
      # username: "postgres"
      # password:
      #   secretRef: postgresql-secrets
      #   secretKey: postgres-password
      # --------------OR----------------
      #   value: password

  datahub:
    version: v0.10.2
    gms:
      port: "8080"
      nodePort: "30001"

    monitoring:
      enablePrometheus: true

    mae_consumer:
      port: "9091"
      nodePort: "30002"

    appVersion: "1.0"
    systemUpdate:
      ## The following options control settings for datahub-upgrade job which will
      ## managed ES indices and other update related work
      enabled: true

    encryptionKey:
      secretRef: "datahub-encryption-secrets"
      secretKey: "encryption_key_secret"
      # Set to false if you'd like to provide your own secret.
      provisionSecret:
        enabled: true
        autoGenerate: true
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    encryptionKey: <encryption key value>

    managed_ingestion:
      enabled: true
      defaultCliVersion: "0.10.0"

    metadata_service_authentication:
      enabled: false
      systemClientId: "__datahub_system"
      systemClientSecret:
        secretRef: "datahub-auth-secrets"
        secretKey: "system_client_secret"
      tokenService:
        signingKey:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_signing_key"
        salt:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_salt"
      # Set to false if you'd like to provide your own auth secrets
      provisionSecrets:
        enabled: true
        autoGenerate: true
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    secret: <secret value>
      #    signingKey: <signing key value>
      #    salt: <salt value>

    ## Enables always emitting a MCL even when no changes are detected. Used for Time Based Lineage when no changes occur.
    alwaysEmitChangeLog: true

    ## Enables diff mode for graph writes, uses a different code path that produces a diff from previous to next to write relationships instead of wholesale deleting edges and reading
    enableGraphDiffMode: true

#  hostAliases:
#    - ip: "192.168.0.104"
#      hostnames:
#        - "broker"
#        - "mysql"
#        - "postgresql"
#        - "elasticsearch"
#        - "neo4j"

## Add below to enable SSL for kafka
#  credentialsAndCertsSecrets:
#    name: datahub-certs
#    path: /mnt/datahub/certs
#    secureEnv:
#      ssl.key.password: datahub.linkedin.com.KeyPass
#      ssl.keystore.password: datahub.linkedin.com.KeyStorePass
#      ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#      kafkastore.ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#
#  springKafkaConfigurationOverrides:
#    ssl.keystore.location: /mnt/datahub/certs/datahub.linkedin.com.keystore.jks
#    ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    kafkastore.ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    security.protocol: SSL
#    kafkastore.security.protocol: SSL
#    ssl.keystore.type: JKS
#    ssl.truststore.type: JKS
#    ssl.protocol: TLS
#    ssl.endpoint.identification.algorithm:

elasticsearch:
  enabled: true   # set this to false, if you want to provide your own ES instance.
  imageTag: "7.17.9"
  fullnameOverride: "elasticsearch-master"

  # If you're running in production, set this to 3 and comment out antiAffinity below
  # Or alternatively if you're running production, bring your own ElasticSearch
  replicas: 1
  minimumMasterNodes: 1
  # Set replicas to 1 and uncomment this to allow the instance to be scheduled on
  # a master node when deploying on a single node Minikube / Kind / etc cluster.
  antiAffinity: "soft"

  # # If you are running a multi-replica cluster, comment this out
  clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"

  # # Shrink default JVM heap.
  esJavaOpts: "-Xmx384m -Xms384m"

  # # Allocate smaller chunks of memory per pod.
  resources:
    requests:
      cpu: "100m"
      memory: "768M"
    limits:
      cpu: "1000m"
      memory: "768M"

  # # Request smaller persistent volumes.
  # volumeClaimTemplate:
  #   accessModes: ["ReadWriteOnce"]
  #   storageClassName: "standard"
  #   resources:
  #     requests:
  #       storage: 100M

# Official neo4j chart uses the Neo4j Enterprise Edition which requires a license
neo4j:
  enabled: false  # set this to true, if you have a license for the enterprise edition
  acceptLicenseAgreement: "yes"
  defaultDatabase: "graph.db"
  neo4jPassword: "datahub"
  # For better security, add password to neo4j-secrets k8s secret and uncomment below
  # existingPasswordSecret: neo4j-secrets
  core:
    standalone: true

# Deploys neo4j community version. Only supports single node
neo4j-community:
  enabled: false   # set this to true, if you want to run neo4j community edition
  acceptLicenseAgreement: "yes"
  defaultDatabase: "graph.db"
  # For better security, add neo4j-secrets k8s secret with neo4j-password and uncomment below
  existingPasswordSecret: neo4j-secrets

mysql:
  enabled: true
  auth:
    # For better security, add mysql-secrets k8s secret with mysql-root-password, mysql-replication-password and mysql-password
    existingSecret: mysql-secrets

postgresql:
  enabled: false
  auth:
    # For better security, add postgresql-secrets k8s secret with postgres-password, replication-password and password
    existingSecret: postgresql-secrets

cp-helm-charts:
  # Schema registry is under the community license
  cp-schema-registry:
    enabled: true
    fullnameOverride: datahub-cp-schema-registry
    kafka:
      bootstrapServers: "datahub-kafka:9092"  # <<release-name>>-kafka:9092
  cp-kafka:
    enabled: false
  cp-zookeeper:
    enabled: false
  cp-kafka-rest:
    enabled: false
  cp-kafka-connect:
    enabled: false
  cp-ksql-server:
    enabled: false
  cp-control-center:
    enabled: false

# Bitnami version of Kafka that deploys open source Kafka https://artifacthub.io/packages/helm/bitnami/kafka
kafka:
  enabled: true
  fullnameOverride: datahub-kafka
  initContainers:
    - name: wait-for-zookeeper
      image: "jwilder/dockerize:latest"
      command:
        - /bin/sh
        - -c
        - |
          dockerize -wait "tcp://{{ .Values.global.kafka.zookeeper.server }}/" -timeout 360s
  zookeeper:
    fullnameOverride: datahub-zookeeper
